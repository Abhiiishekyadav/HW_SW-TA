# -*- coding: utf-8 -*-
"""cifar_100_data_cpp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w2GbCWxYoiF-lv_8u__n9-wJr_bQHBxK

importing libraries
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout
from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import Model

"""dataset generation"""



cifar100 = tf.keras.datasets.cifar100

# train and test set splitting
(x_train, y_train), (x_test, y_test) = cifar100.load_data()
print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)

x_train[0]

#from keras.datasets import mnist
import matplotlib.pyplot as plt

# Load the MNIST dataset
#(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Display the shape of the dataset
print("Training data shape:", x_train.shape)
print("Training labels shape:", y_train.shape)
print("Testing data shape:", x_test.shape)
print("Testing labels shape:", y_test.shape)

# Plot some sample images
plt.figure(figsize=(10, 5))

for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(f"Label: {y_train[i]}")
    plt.axis('off')

plt.show()

# Normalization
x_train, x_test = x_train / 255.0, x_test / 255.0

# flattening
y_train, y_test = y_train.flatten(), y_test.flatten()

"""visualization"""

# plotting images
fig, ax = plt.subplots(5, 5)
k = 0

for i in range(5):
	for j in range(5):
		ax[i][j].imshow(x_train[k], aspect='auto')
		k += 1

plt.show()

"""#Model"""

import tensorflow as tf
from tensorflow.keras import layers, models

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import cifar100

#118,96,64,
#132,84,156,
#128,102,54,96

# Build the CNN model
model = tf.keras.models.Sequential()

# Layer 1: Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(118, (3, 3), activation='relu', input_shape=(32, 32, 3)))


# Layer 2: Convolutional layer with 64 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(96, (3, 3), activation='relu'))

# Layer 3: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

# Layer 4: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(132, (3, 3), activation='relu'))

# Layer 5: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(84, (3, 3), activation='relu'))

# Layer 6: Convolutional layer with 64 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(156, (3, 3), activation='relu'))

# Layer 7: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(128, (3, 3), activation='relu'))

# Layer 8: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(102, (3, 3), activation='relu'))

# Layer 9: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(54, (3, 3), activation='relu'))

# Layer 10: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(96, (3, 3), activation='relu'))

# Layer 11: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
#model.add(layers.Conv2D(861, (3, 3), activation='relu'))

# Layer 12: Convolutional layer with 128 filters, 3x3 kernel size, and ReLU activation
#model.add(layers.Conv2D(956, (3, 3), activation='relu'))


# Flatten the output for the fully connected layers
model.add(layers.Flatten())


# Layer 5: Output layer with 10 neurons (for 10 classes in MNIST) and softmax activation
model.add(layers.Dense(100, activation='softmax'))

# Train the model
#model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_data=(test_images, test_labels))

model.summary()

model.compile(optimizer='adam',
			loss='sparse_categorical_crossentropy',
			metrics=['accuracy'])

"""Training"""

from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping

batch_size = 32
data_generator = tf.keras.preprocessing.image.ImageDataGenerator(
width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)

train_generator = data_generator.flow(x_train, y_train, batch_size)
steps_per_epoch = x_train.shape[0] // batch_size

r = model.fit(train_generator, validation_data=(x_test, y_test),
			steps_per_epoch=steps_per_epoch, epochs=2,callbacks=[EarlyStopping(monitor="val_accuracy",patience=7,verbose=1)])

"""Prediction"""

test_loss, test_accuracy = model.evaluate(x_test,y_test)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

tf.keras.utils.plot_model(model, show_shapes=True, show_trainable=True)

for layer in model.layers:
  print(layer.get_weights())

b = 8
alpha_q = -2**(b-1)
beta_q = (2**(b-1) - 1)
print(alpha_q)
print(beta_q)
for layer in model.layers:
  #print(layer.name)
  if layer.name!='flatten':
    #print("yes")
    #print(layer.get_weights())
    (W,B) = np.array(layer.get_weights())
    #print("bias of", layer.name,B)
    alpha_W = np.min(W)
    beta_W = np.max(W)
    alpha_B = np.min(B)
    beta_B = np.max(B)
    s_W = (beta_W - alpha_W) / (beta_q - alpha_q)
    s_B = (beta_B - alpha_B) / (beta_q - alpha_q)
    z_W = np.round((beta_W*alpha_q - alpha_W*beta_q)/(beta_W - alpha_W))
    z_B = np.round((beta_B*alpha_q - alpha_B*beta_q)/(beta_W - alpha_W))
    #print(W)
    #print(B)
    W1 = np.round(( (W / s_W) + z_W), decimals=0)
    B1 = np.round(( (B / s_B) + z_B), decimals=0)
    #print(W1)
    #print(B1)
    W2 = np.clip(W1, alpha_q, beta_q)
    B2 = np.clip(B1, alpha_q, beta_q)
    #print(W2)
    #print(B2)
    W_quantized = W2.astype('int')
    B_quantized = B2.astype('int')
    #print(W_quantized)
    #print(B_quantized)
    Dist = [W_quantized, B_quantized]
    #print(Dist)
    layer = layer.set_weights(Dist)

(W,B) = np.array(model.layers[1].get_weights())

W = W.flatten()
W

B.flatten()
B

with open('weight_data.txt', 'w') as file:
  for layer in model.layers:
    if layer.name!='flatten':
      file.write(f"Layer Name: {layer.name}\n")
      file.write(f"Layer Shape: {layer.output_shape}\n")
      (W,B) = np.array(layer.get_weights())
      file.write(f"Layer weights: {W.flatten()}\n")
      file.write(f"Layer bias: {B.flatten()}\n")
      file.write("\n")

